<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Anton Sugolov" />
  <title>Multiscale Matrix Decomposition</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../../styles.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<script>
MathJax = {
  tex: {
    macros: {
      R: "\\mathbb{R}",
      N: "\\mathbb{N}",
      Z: "\\mathbb{Z}",
      C: "\\mathbb{C}",
      F: "\\mathbb{F}",
      Q: "\\mathbb{Q}",
      Ex: "\\mathbb{E}",
      PR: "\\mathbb{P}",
      eps: "\\varepsilon",
      lam: "\\lambda",
      delt: "\\delta",
      imp: "\\implies",
      all: "\\forall",
      exs: "\\exists",
      ra: "\\rightarrow",
      ol: "\\overline",
      f: ["\\frac{#1}{#2}", 2],
      df: ["\\dfrac{#1}{#2}", 2],
      abs: ["\\left|#1\\right|", 1],
      norm: ["\\left\\lVert#1\\right\\rVert", 1],
      inn: ["\\langle#1\\rangle", 1],
      lan: "\\langle",
      ran: "\\rangle",
      tr: "\\text{tr}",
      Var: "\\text{Var}",
      Cov: "\\mathrm{Cov}",
      dd: "\\mathrm{d}",
      bmat: ["\\begin{bmatrix}#1\\end{bmatrix}", 1],
      pmat: ["\\begin{pmatrix}#1\\end{pmatrix}", 1]
    }
  }
};
</script>
<header id="title-block-header">
<h1 class="title"><strong>Multiscale Matrix Decomposition</strong></h1>
<p class="author">Anton Sugolov</p>
</header>
<p>In this project, we survey a method for low-rank matrix
decompositions with broad applications throughout signal and image
processing. We focus on proposed by <span class="citation"
data-cites="ong2016"></span> to generalize sparse and low-rank
decomposition to intermediate data scales. An implementation of
multiscale decomposition is provided along with the necessary
prerequisites to derive their formulation.</p>
<h1 id="section:slr">Sparse + Low Rank</h1>
<p>We begin by motivating the results of <span class="citation"
data-cites="ong2016"></span> through the work of <span class="citation"
data-cites="candes2009robustprincipalcomponentanalysis"></span>. Suppose
a given data matrix <span class="math inline">\(Y\)</span> may be
decomposed into a low-rank component <span
class="math inline">\(L\)</span> and a sparse component <span
class="math inline">\(S\)</span> (meaning that the entries are almost
entirely zero, except in several areas that significantly contribute to
the overall rank): <span class="math display">\[Y = L_0 + S_0\]</span>
In the above, the rank of <span class="math inline">\(L\)</span> is not
known, and neither is the number of non-zero elements of <span
class="math inline">\(S\)</span>, which results in a very large number
of unknowns in the problem. Surprisingly, <span class="citation"
data-cites="candes2009robustprincipalcomponentanalysis"></span> were
able to show that <span class="math inline">\(L_0,S_0\)</span> may be
recovered (with high probability) by solving the tractable convex
problem <span class="math display">\[\begin{align*}
    \text{minimize} \, \norm{L}_* + \lambda \norm{S}_1 \quad
\text{subject to} \quad Y = L + S
\end{align*}\]</span> where <span class="math inline">\(\norm{L}_* =
\sum_{i} \sigma_i(L)\)</span>. To make sense of whether a particular
feature belongs to the sparse component or the low-rank component, the
authors additionally introduce an incoherence condition on <span
class="math inline">\(L\)</span> that enforces its singular vectors to
be sufficiently spread out. Since the publication of their work, the
sparse + low rank decomposition has been highly impactful for
decomposing data matrices in many applications. Some examples
anticipated in the original work include</p>
<ol>
<li><p><span><strong>Video surveillance footage.</strong></span> Suppose
<span class="math inline">\(Y\)</span> represents frames of a video
across time. A natural decomposition for this data is <span
class="math inline">\(Y = L + S\)</span> where the low-rank <span
class="math inline">\(L\)</span> is useful for the ambient background
throughout the frames, while <span class="math inline">\(S\)</span>
captures various objects that appear in the video at different
times.</p></li>
<li><p><span><strong>Face recognition.</strong></span> Suppose <span
class="math inline">\(Y\)</span> represents a dataset of faces which is
known to form a low-dimensional surface <span class="citation"
data-cites="ronen2003"></span>. In this case, <span
class="math inline">\(L\)</span> might represent a typical face in the
dataset and <span class="math inline">\(S\)</span> might represent
additional individual changes or shadows added to the image.</p></li>
<li><p><span><strong>Preference identifcation.</strong></span> Suppose
<span class="math inline">\(Y\)</span> represents a dataset of user
preferences on some website. While most users may have the same general
interests <span class="math inline">\(L\)</span>, the sparse <span
class="math inline">\(S\)</span> may capture individual variations that
are predictive of their usage trends.</p></li>
</ol>
<p>Other applications exist across all domains of modern signal
processing. The more recent results of <span class="citation"
data-cites="ong2016"></span> generalize these decompositions to provide
a flexible framework for detecting correlations within data matrices
across a variety of data features.</p>
<h1 id="multiscale">Multiscale</h1>
<p>If <span class="math inline">\(Y \in \R^{M \times N}\)</span>
represents a matrix of data, such as an image with gray scale intensity
values, the entries often have some correlation structure. Multi-scale
decompositions account for correlations with different granularity,
aiming to give a structured decomposition of <span
class="math inline">\(Y\)</span>: for example, detecting edges in a low
scale component, distinct objects in another an intermediate scale, and
a background colour in the low scale. In this section, we overview how
to fit a multiscale decomposition and provide some examples on image
data. Throughout this section we summarize the original contributions of
<span class="citation" data-cites="ong2016"></span>, including helpful
figures from their work.</p>
<figure id="fig:visualexample" data-latex-placement="h!">
<img src="figs/visual_example.png" style="width:90.0%" />
<figcaption> An example of a multiscale decomposition of input <span
class="math inline">\(Y\)</span> into a sum of <span
class="math inline">\(X_1, X_2, X_3, X_4\)</span> where each <span
class="math inline">\(X_i\)</span> reflects the underlying features of
<span class="math inline">\(Y\)</span> with different scales.
</figcaption>
</figure>
<h2 id="overview">Overview </h2>
<p>Consider a data matrix <span class="math inline">\(Y \in \R^{M \times
N}\)</span>. We define the <strong>scales</strong> of <span
class="math inline">\(Y\)</span> as a set of partitions of its entries
<span class="math inline">\(\{P_i\}_{i=1}^n\)</span> which define blocks
of <span class="math inline">\(Y\)</span> (see Figure <a
href="#fig:multiscalepartition" data-reference-type="ref"
data-reference="fig:multiscalepartition">[fig:multiscalepartition]</a>).
Typically, <span class="math inline">\(P_i\)</span> is set to be an
order of magnitude greater than the previous <span
class="math inline">\(P_{i-1}\)</span>. For each block <span
class="math inline">\(b\)</span> defined by scale-level partition <span
class="math inline">\(P_i\)</span>, with block widths <span
class="math inline">\(m_i \times n_i\)</span>, we define the
<em>reshaping operator</em> <span class="math inline">\(R_b\)</span>
which projects <span class="math inline">\(Y\)</span> onto the <span
class="math inline">\(m_i \times n_i\)</span> block <span
class="math inline">\(b\)</span> (see Figure <a
href="#fig:reshapeoperator" data-reference-type="ref"
data-reference="fig:reshapeoperator">[fig:reshapeoperator]</a>). The
transpose operator <span class="math inline">\(R_b^T\)</span> injects
each patch <span class="math inline">\(b\)</span> into a zero matrix
with the same shape as <span class="math inline">\(Y\)</span>.</p>
<figure id="fig:combined" data-latex-placement="htbp">
<div class="minipage">
<img src="figs/multiscale_partition.png" />
</div>
<div class="minipage">
<img src="figs/reshape_operator.png" />
</div>
<figcaption>Reshape operators <span class="math inline">\(R_b\)</span>,
<span class="math inline">\(R_b^T\)</span>.</figcaption>
</figure>
<p>The aim of multiscale decomposition is to write <span
class="math display">\[Y = \sum_{i} X_i
    \text{ such that }
    X_i = \sum_{b \in P_i} R_b^T (U_b S_b V_b^T)\]</span> where <span
class="math inline">\(U_b, S_b, V_b\)</span> form the rank <span
class="math inline">\(r_b\)</span> SVD of the block <span
class="math inline">\(b\)</span>. That is, we write the matrix <span
class="math inline">\(Y\)</span> as a sum of local truncated SVDs of
different scales, aiming to locally reduce the rank of each block.</p>
<p><span><strong>Note.</strong></span> The sparse + low rank
decomposition (Section <a href="#section:slr" data-reference-type="ref"
data-reference="section:slr">1</a>) may be viewed as a 2-scale
decomposition where <span class="math inline">\(P_1\)</span> defines the
whole matrix and <span class="math inline">\(P_2\)</span> defines the
<span class="math inline">\(1 \times 1\)</span> scale. The example of
the video surveillance footage may be extended in the multiscale
framework by adding an additional partition <span
class="math inline">\(P_i\)</span> to detect persistent features at a
custom time scale.<br />
To fit a multiscale decomposition, we set up an appropriate convex
problem whose minimum gives the solution. Consider the motivating
objective <span class="math display">\[\min_{X_1, \ldots, X_L}
\sum_{i=1}^L \sum_{b \in P_i} \text{rank} (R_b(X_i))
    \text{ subject to }
    Y = \sum_{i=1}^L X_i\]</span> Several issues arise with this
approach: (1) the objective is not convex, (2) splitting into patches
makes the sum over <span class="math inline">\(b \in P_i\)</span>
combinatorial in nature and (3) for smaller patches the rank penalty may
be excessive. For example, for an element-wise partition a rank 1
penalty and a 1-sparse matrix carry the same cost. Luckily, an
appropriate convex problem can be set up by relaxing the rank
minimization to minimization in a special norm.</p>
<h2 id="convex-problem">Convex problem</h2>
<p>We define several norms that are useful for setting up a computable
optimization objective, whose minimum is the desired multiscale matrix
decomposition.</p>
<div class="definition">
<p>The <span><strong>Ky-Fan</strong></span> <span
class="math inline">\(k\)</span> norm of a matrix <span
class="math inline">\(X \in \R^{M \times N}\)</span> with singular
values <span class="math inline">\(\{ \sigma_i(X)\}_{1 \leq i \leq
\min\{M,N\}}\)</span> <span class="math display">\[\begin{equation}
        \norm{M}_{\text{KF}, k} = \sum_{i=1}^{k} \sigma_i(M)
\end{equation}\]</span></p>
</div>
<div class="definition">
<p>The <span><strong>nuclear</strong></span> norm of <span
class="math inline">\(X \in \R^{M \times N}\)</span> is <span
class="math inline">\(\norm{X}_\text{nuc} = \norm{X}_{\text{KF},
\min\{M,N\}}\)</span>.</p>
</div>
<div class="definition">
<p>The <span><strong>maximum singular value</strong></span> norm of
<span class="math inline">\(X \in \R^{M \times N}\)</span> is <span
class="math inline">\(\norm{X}_\text{msv} = \norm{X}_{\text{KF},
1}\)</span>.</p>
</div>
<div class="definition">
<p>For block partition <span class="math inline">\(P_i\)</span> of <span
class="math inline">\(X \in \R^{M \times N}\)</span>, the
<span><strong>block-wise nuclear</strong></span> norm of the <span
class="math inline">\(\mathbf{i}\)</span>-<span><strong>th
scale</strong></span> <span class="math display">\[\begin{equation}
            \norm{X}_{(i)} = \sum_{b \in P_i} \norm{R_bX}_\text{nuc}
\end{equation}\]</span> is the sum of the nuclear norms of each patch
<span class="math inline">\(b \in P_i\)</span>.</p>
</div>
<p>The associated convex optimization problem to compute the multiscale
components of the matrix is <span
class="math display">\[\begin{equation}
    \label{convex_problem}
    \text{min}_{X_1, \ldots, X_L} \sum_{i=1}^{L} \lambda_i
\norm{X_i}_{(i)}
    \text{ subject to }
    Y = \sum_{i=1}^{L} X_i
\end{equation}\]</span> typically <span class="math inline">\(\lambda_i
\sim \sqrt{m_i} + \sqrt{n_i} + \sqrt{\log(MN /
\max\{m_i,n_i\})}\)</span> where this heuristic follows from optimal
values for Gaussian random matrices <span class="citation"
data-cites="Bandeira_2016"></span>.<br />
<span><strong>Minimization objective.</strong> </span> We formulate the
problem with the alternating direction method of multipliers, by writing
a separable objective with an equality constraint. For the <span
class="math inline">\(L\)</span> scales corresponding to the block
partitions of <span class="math inline">\(Y\)</span>:<br />
<span class="math display">\[\begin{align*}
\label{OBJ}
    \text{min}_{X_1, \ldots, X_L, Z_1, \ldots, Z_L}
    &amp;\quad  I\left\{Y = \sum_{i=1}^L
X_i\right\}+\sum_{i=1}^{L}  \lambda_i \norm{Z_i}_{(i)} \tag{$\star$}
\\  \text{ subject to }
    &amp;\quad X_i = Z_i
\end{align*}\]</span> where <span class="math inline">\(I\)</span>
represents the (inverse) indicator function and <span
class="math inline">\(\lambda_i\)</span> follows the initialization of
Equation <a href="#convex_problem" data-reference-type="ref"
data-reference="convex_problem">[convex_problem]</a>.</p>
<div class="theorem">
<p><span id="thm:guarantee" data-label="thm:guarantee"></span> Consider
the vector space of matrices with the same block-wise row space as the
scale <span class="math inline">\(X_i\)</span>: <span
class="math display">\[\begin{equation*}
        T_i = \left\{ \sum_{b \in P_i} R_b^{\top} \left(U_b X_b^{\top} +
Y_b V_b^{\top}\right) : X_b \in \mathbb{C}^{n_i \times r_i}, Y_b \in
\mathbb{C}^{m_i \times r_i} \right\}
\end{equation*}\]</span> Let <span class="math inline">\(\mu_{ij} = \max
\{\|N_j\|_{(i)}^*\, \mid \, N_j \in T_j,\,  \|N_j\|_{{(j)}}^* \leq
1  \}\)</span> where <span class="math inline">\(\norm{\, \cdot \,
}_{(i)}^* = \max_{b \in P_i} \norm{R_b(\cdot)}_{\text{msv}}\)</span> is
the dual norm associated to the block-wise nuclear norm. If
regularization parameters <span
class="math inline">\(\lambda_{i}\)</span> can be chosen such that <span
class="math display">\[\sum_{j\neq i} \mu_{ij} \f{\lambda_j}{\lambda_i}
&lt; \f12\]</span> then the convex problem (<a href="#OBJ"
data-reference-type="ref" data-reference="OBJ">[OBJ]</a>) has a unique
solution <span class="math inline">\(\{X_i\}_{i=1}^L\)</span> which is
the desired multiscale decomposition.</p>
</div>
<p>Theorem <a href="#thm:guarantee" data-reference-type="ref"
data-reference="thm:guarantee">[thm:guarantee]</a> guarantees that the
minimization (<a href="#OBJ" data-reference-type="ref"
data-reference="OBJ">[OBJ]</a>) has the desired solution. We omit the
proof since it is technical (requiring several facts about <span
class="math inline">\(T_i\)</span> and the dual block-wise nuclear
norm), and proceed with obtaining the solution <span
class="math inline">\(\{X_i\}_{i=1}^L\)</span>. To compute this
decomposition, the authors propose the <em>alternating direction method
of multipliers</em> <span class="citation"
data-cites="boyd_2011"></span>, which we now briefly overview.</p>
<h2 id="alternating-direction-method-of-multipliers">Alternating
direction method of multipliers</h2>
<p>For the sake of completion, we briefly summarize the method of dual
ascent and method of multipliers. Afterward, we discuss the alternating
direction method of multipliers (ADMM) and apply it to the matrix
factorization problem at hand.</p>
<ol>
<li><p>The <span><strong>method of dual ascent</strong></span> for
convex differentiable <span class="math inline">\(f\)</span> (p. 529,
Gallier Quaintance Vol. II) is posed as <span
class="math display">\[\begin{align}
\label{eqn:default_convex}
    \text{min}_{x}
    \,  f(x) \quad  \text{ subject to }
    \quad Ax  = b
\end{align}\]</span> The associated Lagrangian is then <span
class="math inline">\(L(x,\lambda) = f(x) + \lambda^T(Ax-b)\)</span>. By
defining the <span><strong>convex conjugate</strong></span> <span
class="math inline">\(f^*(y) = \sup_{x \in U} (y^Tx - f(x)),\, y\in
\R^n\)</span>, the dual formulation gives a unique <span
class="math inline">\(\lambda \in \R^m\)</span> and <span
class="math inline">\(x_\lambda \in \R^n\)</span> with <span
class="math display">\[G(\lambda) = L(x_\lambda, \lambda) = \inf_{x \in
\R^n} L(x, \lambda)\]</span> If <span class="math inline">\(\lambda \to
x_\lambda\)</span> is continuous, then <span
class="math inline">\(G\)</span> is differentiable, and <span
class="math inline">\(\nabla G_\lambda = Ax_\lambda - b\)</span> for any
solution of the dual problem. The dual ascent method therefore becomes
gradient ascent applied to the dual function <span
class="math inline">\(G\)</span>, and is given by the update steps <span
class="math display">\[\begin{align*}
    x^{k+1} &amp;= \arg\min_x L(x, \lambda_k)
\\  \lambda^{k+1} &amp;= \lambda^k + \alpha^k (Ax^{k+1} - b)
\end{align*}\]</span> <span class="math inline">\(\alpha^k\)</span> is
some step size, which is difficult to determine in this case.</p></li>
<li><p>The <span><strong>method of multipliers</strong></span> is
formulated for the same convex problem (Equation <a
href="#eqn:default_convex" data-reference-type="ref"
data-reference="eqn:default_convex">[eqn:default_convex]</a>), except
the Lagrangian is augmented by an addition penalty <span
class="math display">\[L_\rho(x,\lambda) = f(x) + \lambda^T(Ax-b) +
\f\rho{2} \norm{Ax-b}_2^2\]</span> where <span
class="math inline">\(\rho\)</span> is a certain penalty parameter.
Applying the method of dual descent to <span
class="math inline">\(L_\rho\)</span> yields update steps <span
class="math display">\[\begin{align*}
    x^{k+1} &amp;= \arg\min_x L_\rho(x, \lambda_k)
\\ \lambda^{k+1} &amp;= \lambda^k + \rho (Ax^{k+1} - b)
\end{align*}\]</span> In particular, <span
class="math inline">\(\alpha^k = \rho\)</span> is able to be determined.
Under some mild conditions on <span class="math inline">\(A\)</span>,
the above can be shown to converge to a unique solution, which is in
contrast to simple dual ascent (p. 533, Gallier Quaintance Vol.
II).</p></li>
<li><p>The <span><strong>alternating direction method of
multipliers</strong></span> was proposed by <span class="citation"
data-cites="boyd_2011"></span> for separable optimization objectives
subject to an equality constraint <span
class="math display">\[\begin{align*}
\label{obj_admm}
    \text{min}_{x,z}
    &amp;\quad  f(x) + g(z) \tag{$\star$}
\\ \text{ subject to }
    &amp;\quad Ax + Bz = c
\end{align*}\]</span> where <span class="math inline">\(f,g\)</span> are
convex, <span class="math inline">\(x \in \R^n\)</span>, <span
class="math inline">\(z \in \R^m\)</span>, <span class="math inline">\(c
\in \R^p\)</span>, and <span class="math inline">\(A \in \R^{p \times
n}, B \in \R^{p \times m}\)</span>. The problem can be formulated
similarly to the method of multipliers (p. 533, Gallier Quaintance Vol.
II), where the constraint is enforced with an augmented Lagrangian <span
class="math display">\[L_\rho(x,z,u) = f(x) + g(z) + u^T(Ax +Bz - c) +
\f\rho2 \norm{Ax + Bz - c}_2^2\]</span> The update steps are implemented
via dual ascent (p. 529, Gallier Quaintance Vol. II) for both <span
class="math inline">\(x,z\)</span>: <span
class="math display">\[\begin{align*}
    x^{k+1} &amp;= \arg\min_x L\rho (x, z^k, u^k)
\\ z^{k+1} &amp;= \arg\min_z L\rho (x^{k+1}, z, u^k)
\\ u^{k+1} &amp;= u^k + \rho(Ax^{k+1} + Bz^{k+1} - c)
\end{align*}\]</span> Note that the dual update is done after the <span
class="math inline">\(z\)</span>-update but before the <span
class="math inline">\(x\)</span>-update, so the roles of <span
class="math inline">\(x,z\)</span> are not quite symmetric.
Interestingly, these alternating updates can be viewed as a type of
Gauss-Seidel pass over <span class="math inline">\(x,z\)</span> instead
of the typical joint update of <span
class="math inline">\((x,z)\)</span>.</p></li>
</ol>
<p>Returning to the objective (<a href="#OBJ" data-reference-type="ref"
data-reference="OBJ">[OBJ]</a>), <span
class="math display">\[L_\rho(X,Z,U) = I\left\{Y = \sum_{i=1}^L
X_i\right\}+\sum_{i=1}^{L}  \lambda_i \norm{Z_i}_{(i)} + U^T(X-Z) +
\f\rho2 \norm{X-Z}_2^2\]</span> To compute the updates for the
multiscale objective (<a href="#OBJ" data-reference-type="ref"
data-reference="OBJ">[OBJ]</a>), it suffices to compute the <em>proximal
operators</em> <span class="math display">\[\arg\min_X L_\rho(X, Z, U)
\qquad \arg\min_Z L_\rho(X, Z, U)\]</span> of the corresponding <span
class="math inline">\(I\left\{Y = \sum_{i=1}^L X_i\right\}\)</span>,
<span class="math inline">\(\sum_{i=1}^{L} \lambda_i
\norm{Z_i}_{(i)}\)</span>. For the indicator, it is simply the
projection operator to the set. For the block-wise nuclear norm, <span
class="citation" data-cites="ong2016"></span> show that the proximal
steps are given by the <span><strong>singular value
threshhold</strong></span> and for the scale regularizer <span
class="math inline">\(\lambda_i\)</span>.</p>
<div class="definition">
<p>Given a regularization <span class="math inline">\(\lambda\)</span>
and <span class="math inline">\(X \in \R^{m \times n}\)</span>, the
<span><strong>singular value threshhold</strong></span> is <span
class="math display">\[\begin{equation}
        \text{SVT}(X, \lambda) = U \max(\Sigma - \lambda, 0) V^T
\end{equation}\]</span> where <span class="math inline">\(X=U\Sigma
V^T\)</span> is the SVD of <span class="math inline">\(X\)</span>, and
it is understood that <span
class="math inline">\(\Sigma-\lambda\)</span> is taken componentwise. In
other words we modify the SVD so that for each singular value <span
class="math inline">\(\sigma_i\)</span>, we reset <span
class="math inline">\(\sigma_i&#39; = \max(\sigma_i - \lambda,
0)\)</span>.</p>
</div>
<div class="definition">
<p>Given a regularization <span class="math inline">\(\lambda_i\)</span>
and <span class="math inline">\(X \in \R^{m \times n}\)</span>, with
partition <span class="math inline">\(P_i\)</span> of scale <span
class="math inline">\(i\)</span>, the <span><strong>block-wise singular
value threshhold</strong></span> is <span
class="math display">\[\begin{equation}
        \text{BlockSVT}(X, \lambda_i) = \sum_{b\in P_i} R_b^T \,
\text{SVT}( R_b(X), \lambda_i)
\end{equation}\]</span> That is, we simply threshhold each block in the
partition by <span class="math inline">\(\lambda_i\)</span>.</p>
</div>
<p>Combining the above discussion, the algorithm to compute the
multiscale matrix decomposition is given by Algorithm <a href="#algo"
data-reference-type="ref" data-reference="algo">[algo]</a>. For the sake
of summary, we omit certain details from the original work <span
class="citation" data-cites="ong2016"></span>.</p>
<figure id="fig:iterations" data-latex-placement="h!">
<div class="minipage">
<div class="algorithm">
<div class="algorithmic">
<p><span id="algo" data-label="algo"></span> Image <span
class="math inline">\(Y\)</span>, block sizes <span
class="math inline">\(\{m_i, n_i\}_{i=1}^L\)</span>, parameters <span
class="math inline">\(\{\lambda_i\}_{i=1}^L\)</span>, <span
class="math inline">\(\rho\)</span> <span
class="math inline">\(X_i^{(0)}, Z_i^{(0)}, U_i^{(0)} \leftarrow
0\)</span> for <span class="math inline">\(i = 1,\ldots,L\)</span> <span
class="math inline">\(X_i^{(k)} \leftarrow (Z_i^{(k-1)} - U_i^{(k-1)}) +
\frac{1}{L}\left(Y - \sum_{j=1}^L(Z_j^{(k-1)} -
U_j^{(k-1)})\right)\)</span> <span class="math inline">\(Z_i^{(k)}
\leftarrow \text{BlockSVT}\left(X_i^{(k)} + U_i^{(k-1)},
\lambda_i/\rho\right)\)</span> <span class="math inline">\(U_i^{(k)}
\leftarrow U_i^{(k-1)} - (Z_i^{(k)} - X_i^{(k)})\)</span> <span
class="math inline">\(\{X_i^{(K)}\}_{i=1}^L\)</span></p>
</div>
</div>
</div>
<div class="minipage">
<img src="figs/iterations.png" />
</div>
<figcaption>A heuristic representation of the ADMM approach to obtain a
multiscale decomposition.</figcaption>
</figure>
<h1 id="results">Results</h1>
<p>We implement Algorithm <a href="#algo" data-reference-type="ref"
data-reference="algo">[algo]</a> in JAX and evaluate on a simple test
case. The results when <span class="math inline">\(\rho=0.5\)</span> and
<span class="math inline">\(K=200\)</span> are summarized in Figure <a
href="#fig:catdecomp" data-reference-type="ref"
data-reference="fig:catdecomp">4</a>. For the exact implementation
details, see the attached the Jupyter notebook, or the implementation on
Google Colab:</p>
<div class="center">
<p><span><u><a
href="https://colab.research.google.com/drive/1Xic21fLrptruF29gM3CHUOBcYxeGjT_3?usp=sharing">Google
Colab (https://tinyurl.com/389ddhps)</a></u></span></p>
</div>
<figure id="fig:catdecomp" data-latex-placement="h!">
<img src="figs/cat_decomp_0.5_200.jpg" style="width:95.0%" />
<figcaption>Multiscale decomposition of a cat image (top left) using the
(Algorithm <a href="#algo" data-reference-type="ref"
data-reference="algo">[algo]</a>) with <span class="math inline">\(\rho
= 0.5\)</span> and <span class="math inline">\(K=200\)</span> iterations
using scales <span class="math inline">\(\{1, 2, 4, \ldots,
64\}\)</span>. Scale-level features are apparent after multiscale
decomposition; <span class="math inline">\(1\times 1\)</span> accurately
capture the edge features of the image, <span
class="math inline">\(16\times 16\)</span> highlight the hearts present
in the original, while <span class="math inline">\(64 \times 64\)</span>
captures a silhouette of the cat. </figcaption>
</figure>
<p>The results of Figure <a href="#fig:catdecomp"
data-reference-type="ref" data-reference="fig:catdecomp">4</a>
demonstrate that the ADMM approach indeed captures certain scale-level
features. Some scales are somewhat noisy, and would likely improve from
more iterations and a larger image size: the original authors use <span
class="math inline">\(256 \times 256\)</span> images with <span
class="math inline">\(\rho = 0.5, K=1024\)</span>. Squares of certain
patch resolution are visible in the decomposition in other components,
which is due to the fixed partition throughout the iterations. <span
class="citation" data-cites="ong2016"></span> propose incorporating a
shift operator to threshholding steps <span
class="math display">\[\begin{equation*}
    Z_i^{(k)} \leftarrow \frac{1}{|S|} \sum_{s \in S}
\text{SHIFT}_{-s}\left(\text{BlockSVT}_{\lambda_i/\rho}\left(\text{SHIFT}_s(X_i^{(k)}
+ U_i^{(k-1)})\right)\right)
\end{equation*}\]</span> where <span class="math inline">\(s \in
S\)</span> is randomly chosen from some a possible set of translations.
This approach would likely lead to improved quality in the
decomposition, reducing the graininess present in the different
components. More refined components may be selected to better capture
the significant features in the image; for example, the <span
class="math inline">\(2 \times 2\)</span> and <span
class="math inline">\(4 \times 4\)</span>, <span
class="math inline">\(32 \times 32\)</span> scales are less informative
in Figure <a href="#fig:catdecomp" data-reference-type="ref"
data-reference="fig:catdecomp">4</a>.</p>
<h1 id="summary">Summary</h1>
<p>In this project, we introduced multiscale matrix factorizations <span
class="citation" data-cites="ong2016"></span>, which generalize sparse
and low-rank decompsitions to a general feature detection setting. The
prerequisites to define the convex problem were set forth in order to
define the ADMM objective, which we implemented in JAX to solve the
multiscale matrix decomposition. After running on a test case, the
possible modifications that could improve the decomposition were
discussed, demonstrating the flexibility of multiscale decomposition to
extract desired data features.</p>
</body>
</html>
