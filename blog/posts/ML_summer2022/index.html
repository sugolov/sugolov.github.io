<!DOCTYPE html>
<html>
<link href="/style.css" rel="stylesheet">
<link rel="icon" href="/favicon.ico">
<head>
    <meta charset="UTF-8">
    <title>Machine learning experiments in summer 2022</title>
</head>

<body>

<div class=title>
    <h1 align=center>
        Machine/statistical learning self study
    </h1>
    <h4 align="center">
        <i>
            July 27th, 2022
        </i>
    </h4>
</div>
<p>
    This blog post is to keep me accountable in completing what I set out to do. Notebooks, code, plots where
    applicable. If there's anything to take away from this blog post, it's to check out:
    <center>
        <a href="https://www.youtube.com/channel/UCm5mt-A4w61lknZ9lCsZtBw">Steve Brunton's YouTube Channel</a>
    </center>
    <h3>Sections</h3>
    <ul>
        <li>Q-Learning</li>
        <li>Understanding why neural networks are universal function approximators</li>
        <li>Learning non-linear dynamics</li>
    </ul>
    <h3>Summary</h3>
    This summer I played around with some ML projects. My initial motivation was to
    understand reinforcement learning. Initially updating Q tables via the typical Bellman equation, and understanding that
    a deep Q neural network is an approximation of a Q function, I became interested in the universal function
    approximation theorem. This lead to realizing the power of a network to predict state to state transitions
    in a dynamical system, and help solve optimal control problems. While taking STA303 and reading Burton and Kuntz
    <i>Machine Learning, Dynamical Systems, and Control</i> I became interested in variable selection for sparse identification
    of nonlinear dynamics. At the end of the post, a neural net predicts the dynamics of the Lorenz system. I'll also try
    to do something with weather measurements; the freely available dynamical system sensitive to initial conditions.

</p>