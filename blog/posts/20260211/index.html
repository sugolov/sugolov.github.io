<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Owen L., Anton S." />
  <meta name="dcterms.date" content="2026-02-11" />
  <title>jepax v0: an implementation of IJEPA-B training in JAX/Equinox</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../../styles.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<a href="../.." class="nav-link">‚Üê blog</a>
<header id="title-block-header">
<h1 class="title">jepax v0: an implementation of IJEPA-B training in
JAX/Equinox</h1>
<p class="author"><a href="https://lockwo.github.io/">Owen L</a>., Anton
S.</p>
<p class="date">2026-02-11</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#links" id="toc-links">Links</a></li>
<li><a href="#overview" id="toc-overview">Overview</a></li>
</ul>
</nav>
<p><em>This post is a work in progress!</em></p>
<h2 id="links">Links</h2>
<p><strong>Repository:</strong> <a
href="https://github.com/sugolov/jepax">github.com/sugolov/jepax</a></p>
<h2 id="overview">Overview</h2>
<p>A little while ago, Owen and I got interested in JEPAs and the
self-supervised approach to learning good latent representations. One
theme in ongoing JEPA work are new loss regularizers: the training
setups are similar but with small augmentations to the loss that improve
stability or representation properties. We set out to make
<code>jepax</code> a <a href="https://github.com/google/jax">JAX</a>/<a
href="https://github.com/patrick-kidger/equinox">Equinox</a>
implementation of the self-supervised method, with the goal of a simple
and modifiable codebase that enables fast iteration.</p>
<img src="images/ijepa_b.png" />
<p align="center">
<em><b>Figure:</b> Training loss and linear probe accuracy for IJEPA-B
trained for 300 epochs on 8xA100.</em>
</p>
<p>For this first release, <code>jepax v0</code>, we focused on (1)
1-to-1 configs, losses, and logging with the original PyTorch
implementation and (2) a reproduction of IJEPA-B with data
parallelization on 8xA100. I think we collected a lot of interesting
insights about JEPA training, which I want to describe in this blog.
Some of the themes to discuss:</p>
<ol type="1">
<li>Background on JEPA and IJEPA</li>
<li>Interesting failure modes
<ol type="1">
<li>Smooth <span class="math inline">\(L_1\)</span> loss and
unnormalized <span class="math inline">\(L_2\)</span></li>
<li>Target normalization</li>
</ol></li>
<li>JAX specific considerations</li>
</ol>
<p>Stay tuned!</p>
</body>
</html>
